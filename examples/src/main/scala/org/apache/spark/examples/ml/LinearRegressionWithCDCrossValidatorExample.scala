/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

import org.apache.spark.{ SparkConf, SparkContext, Logging }
import org.apache.spark.examples.mllib.AbstractParams
import org.apache.spark.ml.evaluation.RegressionEvaluator
import org.apache.spark.ml.regression.{ LinearRegressionWithCD, LinearRegressionWithCDModel, ModelLogger }
import org.apache.spark.ml.tuning.{ CrossValidator, AutoGeneratedParamGridBuilder }
import org.apache.spark.mllib.util.LinearDataGenerator
import org.apache.spark.sql.{ SQLContext, Row, DataFrame }
import scopt.OptionParser

/**
 * An example demonstrating linear regression model selection using CrossValidator and 
 * coordinate descent multi-model generation with auto-generated lambda parameter values.
 * 
 * Run with
 * {{{
 * bin/run-example ml.LinearRegressionWithCDCrossValidatorExample
 * }}}
 */
// From spark/examples/src/main/scala/org/apache/spark/examples/ml/CrossValidatorExample.scala
object LinearRegressionWithCDCrossValidatorExample extends Logging {

  case class Params(
    elasticNetParam: Seq[Double] = Seq(0.01),
    numLambdas: Int = 100,
    maxIter: Int = 100,
    tol: Double = 1E-3,
    lambdaShrink: Double = 1E-3,
    fitIntercept: Boolean = true,
    numFolds: Int = 2) extends AbstractParams[Params]

  object ScoptV330 {
    // Provide csv read functionality from scopt 3.3.0 in Spark scopt version 3.2.0
    import scopt.Read
    import scopt.Read.reads
    val sep = ","
    implicit def seqRead[A: Read]: Read[Seq[A]] = reads { (s: String) =>
      s.split(sep).map(implicitly[Read[A]].reads)
    }
  }

  def main(args: Array[String]) {
    import ScoptV330._

    val defaultParams = Params()

    val parser = new OptionParser[Params]("LinearRegression") {
      head("LinearRegression: an example app for linear regression.")
      opt[Seq[Double]]("elasticNetParam")
        .text("value of alpha")
        .action((x, c) => c.copy(elasticNetParam = x))
      opt[Int]("numLambdas")
        .text("number of lambdas")
        .action((x, c) => c.copy(numLambdas = x))
      opt[Int]("maxIter")
        .text("max number of iterations in beta loop")
        .action((x, c) => c.copy(maxIter = x))
      opt[Double]("tol")
        .text("tolerance for beta loop")
        .action((x, c) => c.copy(tol = x))
      opt[Double]("lambdaShrink")
        .text("value of lambda shrink parameter")
        .action((x, c) => c.copy(lambdaShrink = x))
      opt[Boolean]("fitIntercept")
        .text("fit intercept if True")
        .action((x, c) => c.copy(fitIntercept = x))
      opt[Int]("numFolds")
        .text("number of folds for cross validation")
        .action((x, c) => c.copy(numFolds = x))
    }

    parser.parse(args, defaultParams).map { params =>
      run(params)
    } getOrElse {
      sys.exit(1)
    }
  }

  def run(params: Params) {

    val conf = new SparkConf().setAppName("LinearRegressionCrossValidatorExample")
    val sc = new SparkContext(conf)

    val (training, test) = generateData(sc, 1000, 10, 0.1, 6.2, 0.2)

    val lr = new LinearRegressionWithCD("")
      .setElasticNetParam(params.elasticNetParam(0))
      .setNumLambdas(params.numLambdas)
      .setMaxIter(params.maxIter)
      .setTol(params.tol)
      .setLambdaShrink(params.lambdaShrink)
      .setFitIntercept(params.fitIntercept)

    val paramGrid = new AutoGeneratedParamGridBuilder()
      .addGrid(lr.elasticNetParam, params.elasticNetParam)
      .buildWithAutoGeneratedGrid("lambdaIndex", params.numLambdas)

    val crossval = (new CrossValidator(""))
      .setEstimator(lr)
      .setEvaluator(new RegressionEvaluator)
      .setEstimatorParamMaps(paramGrid)
      .setNumFolds(params.numFolds) // Use 3+ in practice

    // Run cross-validation, and choose the best set of parameters.
    val cvModel = crossval.fit(training)
    ModelLogger.logInfo(s"Best Model:\n${cvModel.bestModel.explainParams}")
    val bestModel = cvModel.bestModel.asInstanceOf[LinearRegressionWithCDModel]
    ModelLogger.logInfo(s"Order of model weights: [${bestModel.orderOfWeights.mkString(",")}]")
    ModelLogger.logInfo(s"Model weights: ${bestModel.weights}")

    // Make predictions on test data. cvModel uses the best model found (lrModel).
    val predictions = cvModel.transform(test)
    val eval = new RegressionEvaluator()
    //"mse", "rmse", "r2", "mae"
    EvalLogger.logInfo(s"Test MSE: ${eval.setMetricName("mse").evaluate(predictions)}")
    EvalLogger.logInfo(s"Test R2: ${eval.setMetricName("r2").evaluate(predictions)}")

    sc.stop()
  }

  /**
   * Generate a tuple of DataFrames (training, test) consisting of RDD[LabeledPoint] containing sample data for Linear Regression models.
   *
   * @param sc SparkContext to be used for generating the RDD.
   * @param nexamples Number of examples that will be contained in the RDD.
   * @param nfeatures Number of features to generate for each example.
   * @param eps Epsilon factor by which examples are scaled.
   * @param intercept Intercept.
   * @param fracTest Fraction of data to hold out for testing. Default value is 0.2.
   * @param nparts Number of partitions in the RDD. Default value is 2.
   *
   * @return Tuple of DataFrames (training, training) consisting of RDD[LabeledPoint] containing sample data.
   */
  private def generateData(
    sc: SparkContext,
    nexamples: Int,
    nfeatures: Int,
    eps: Double = 0.1,
    intercept: Double = 0.0,
    fracTest: Double = 0.2,
    nparts: Int = 2): (DataFrame, DataFrame) = {
    val sqlContext = new SQLContext(sc)
    import sqlContext.implicits._

    val data = LinearDataGenerator.generateLinearRDD(sc, nexamples, nfeatures, eps, nparts, intercept)
      .randomSplit(Array(1.0 - fracTest, fracTest), seed = 12345)

    val dataFrames = (data(0).toDF(), data(1).toDF())
    logInfo(s"Generated data: [numExamples: ${nexamples}, numFeatures: ${nfeatures}, eps: ${eps}, intercept: ${intercept}, fracTest: ${fracTest}, training numExamples: ${dataFrames._1.count}, test numExamples: ${dataFrames._2.count}]")
    dataFrames
  }
}

object EvalLogger extends Logging {
  override def logInfo(msg: => String) = { super.logInfo(msg) }
}
// scalastyle:on println
